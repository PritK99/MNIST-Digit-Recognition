# MNIST-Digit-Recognition

## Table of Contents

## Description
### Approach 1: Using Vanilla Neural Networks

#### Method 1: Using SGD learning algorithm

##### Description

Implementing <b>stochastic gradient descent</b> learning algorithm for a vanilla neural network. Here, the gradients are calculated using backpropagation algorithm. The model uses the <b>quadratic cost function (Mean Squared Error)</b> and the sigmoid activation function for all the layers except the input layer.

##### Usage

```python vanilla_neural_network.py```

##### Results

* Using two hidden layers with 100 and 30 neurons respectively
<img src = "assets/nn-with-100-30-hidden-layers.png">
<img src = "assets/vanilla-neural-network.png">


## References
* <a href = "http://neuralnetworksanddeeplearning.com/index.html" >Neural Network and Deep Learning</a> by Michael Nielsen